---
title: Post-Election Reflection
output:
  html_document:
    theme: "cerulean"
---
### November 23, 2020

```{r packages, include=FALSE}

knitr::opts_chunk$set(echo = FALSE)

{
  library(tidyverse)
  library(plotly)
  library(ggpubr)
  library(htmlwidgets)
  library(googlesheets4)
  library(janitor)
  library(knitr)
  library(kableExtra)
  library(lubridate)
}


{
  monochrome <- c('#760000', '#BE1E26', '#D84742', '#FF6B61', '#FF9586')
  primary <- c('#EE3838', '#FA9E1C', '#78C4D4', '#4B5973', '#E2DDDB')
  my_red <- '#BE1E26'
  my_blue <- '#4B5973'
  sidebysidebarplot <- c("#ef3e3e", "#2c3e50")
  theme_hodp <- function () { 
    theme_classic(base_size=12, base_family="Helvetica") %+replace%
      theme(
        panel.background  = element_blank(),
        plot.background = element_blank(),
        legend.background = element_rect(fill="transparent", colour=NA),
        legend.key = element_rect(fill="transparent", colour=NA),
        plot.title = element_text(size=24,  family="Helvetica", face = "bold", 
                                  margin = margin(t = 0, r = 0, b = 10, l = 0)),
        plot.subtitle = element_text(size=18,  family="Helvetica", color="#717171", face = "italic", 
                                     margin = margin(t = 0, r = 0, b = 10, l = 0)),
        plot.caption = element_text(size=8,  family="Helvetica", hjust = 1),
        axis.text.x =element_text(size=10,  family="Helvetica"),
        axis.title.x =element_text(size=14, family="Helvetica", margin = margin(t = 10, r = 0, b = 0, l = 0), 
                                   face = "bold"),
        axis.title.y = element_text(margin = margin(t = 0, r = 10, b = 0, l = 0), size=14, 
                                    family="Helvetica", angle=90, face ='bold'),
        legend.title=element_text(size=10, family="Helvetica"), 
        legend.text=element_text(size=10, family="Helvetica"),
        legend.position = "bottom",
        axis.ticks = element_blank()
      )
    }
}

```

```{r data, message=FALSE, warning=FALSE}

# reading in data with my forecasts

state_pred_compare <- read_csv("../../data/pred_compare.csv")
sims <- read_csv("../../data/election_simulation_results.csv")

# putting all errors together

errors <- read_csv("../../data/error_538.csv") %>% 
  mutate(source = "538") %>% 
  bind_rows(read_csv("../../data/error_economist.csv") %>% mutate(source = "economist"),
            state_pred_compare %>% select(state, diff) %>% rename("error" = diff) %>% 
              mutate(source = "me")) %>% 
  mutate(state = state.abb[match(state, state.name)])

# for maps

{
  us_map <- map_data("state") %>% 
  mutate(region = toupper(region),
    region = state.abb[match(region,  toupper(state.name))])
}

# data that Prof. Enos told us to use

{
 enos_data <- read_sheet("https://docs.google.com/spreadsheets/d/1faxciehjNpYFNivz-Kiu5wGl32ulPJhdJTDsULlza5E/edit#gid=0", 
           col_types = paste0("dcc", paste0(rep("d", times = 39), collapse = ""), collapse = "")) %>% 
  slice(-1) %>% 
  unnest(FIPS) %>% 
  clean_names() %>% 
  rename("democrat" = joseph_r_biden_jr,
         "republican" = donald_j_trump,
         "state" = geographic_name) %>% 
  select(state, democrat, republican) 
  
  enos_pv2p <- enos_data %>% 
  mutate(democrat = democrat / (democrat + republican) * 100,
         republican = 100 - democrat,
         state = state.abb[match(state, state.name)]) %>% 
  pivot_longer(2:3, names_to = "party", values_to = "actual_pv2p")

  pred <- read_csv("../../data/election_simulation_results.csv") %>% 
    drop_na() %>% 
    group_by(state) %>% 
    mutate(d_pv2p = sim_dvotes_2020 / (sim_rvotes_2020 + sim_dvotes_2020),
           r_pv2p = 1 - d_pv2p) %>% 
    summarise(d_pv2p = mean(d_pv2p),
              r_pv2p = mean(r_pv2p),
              d_margin = d_pv2p - r_pv2p) %>% 
    select(1:3) %>% 
    pivot_longer(d_pv2p:r_pv2p, names_to = "party", values_to = "pred_pv2p") %>% 
    mutate(party = recode(party, d_pv2p = "democrat",
                          r_pv2p = "republican"),
           pred_pv2p = pred_pv2p * 100)
  
  enos_pred_compare <- enos_pv2p %>% 
    inner_join(pred, by = c("state", "party")) %>% 
    mutate(diff = actual_pv2p - pred_pv2p) %>% 
    filter(party == "democrat") 
}

# data with changes over time in each state and most recent numbers in each state

{
  dat <- read_csv(
    "https://raw.githubusercontent.com/alex/nyt-2020-election-scraper/master/all-state-changes.csv"
  ) %>% 
    mutate(state = str_replace(state, " \\(.*\\)", ""),
           state = state.abb[match(state, state.name)],
           timestamp = ymd_hms(timestamp),
           pct_reported = precincts_reporting / precincts_total * 100) %>% 
    mutate(trump_votes = case_when(leading_candidate_name == "Trump" ~ leading_candidate_votes,
                                   TRUE ~ trailing_candidate_votes),
           biden_votes = case_when(leading_candidate_name == "Biden" ~ leading_candidate_votes,
                                   TRUE ~ trailing_candidate_votes),
           trump_pv2p = trump_votes / (biden_votes + trump_votes),
           biden_pv2p = biden_votes / (biden_votes + trump_votes)) 
  
  recent_states <- dat %>% 
    select(state, timestamp, trump_votes, biden_votes) %>% 
    group_by(state) %>% 
    arrange(desc(timestamp)) %>% 
    slice(1) %>% 
    ungroup()
  
  changes <- dat %>% 
    pivot_longer(cols = trump_pv2p:biden_pv2p, names_to = "candidate", values_to = "pv2p") %>% 
    mutate(candidate = recode(candidate, "biden_pv2p" = "Biden",
                              "trump_pv2p" = "Trump"))
}

```

```{r nat_pv2p}

# calculating the actual national two-party pv

nat_pv2p <- recent_states %>% 
  summarise(r = sum(trump_votes),
            d = sum(biden_votes),
            .groups = "drop") %>% 
  summarise(d_pv2p = d / (d + r),
            r_pv2p = r / (d + r),
            .groups = "drop")

actual_d_pv2p <- nat_pv2p %>% 
  pull(d_pv2p)

actual_d_pv2p <- round(actual_d_pv2p * 100, 1)

```

### Recap

Just days before the election, my [final forecast](https://kayla-manning.github.io/gov1347/posts/final.html) went against the wisdom of professional forecasters and pollsters alike and projected a rail-thin electoral margin for Joe Biden. While the election results surprised many people on the night of November 3, my model's point prediction anticipated an even closer race in the electoral college--273 electoral votes for Biden compared to his actual 306--but a wider spread in the popular vote--52.8% compared to his actual `r actual_d_pv2p`%.

### Accuracy and Patterns

The statistical aphorism that "all models are wrong, but some are useful" served as my guiding philosophy in constructing this model. As I discussed in my [final prediction](https://kayla-manning.github.io/gov1347/posts/final.html), I did not expect this model to perfectly forecast all outcomes in the election. Rather, this forecast aimed to provide a range of state-level probabilities and outcomes. Then, I used the most probable state-level outcomes to produce point predictions for the Electoral College and national popular vote. While these numbers could be interpreted as my "final prediction", I would have been incredibly shocked if the exact outcomes matched perfectly due to the wide amount of variability in my simulations.

```{r simulated_outcomes}

# seeing how many times this exact cocktail of wins happened

set.seed(9)

ev_uncertainty <- sims %>% 
  mutate(biden_win = ifelse(sim_dvotes_2020 > sim_rvotes_2020, 1, 0)) %>% 
  group_by(state, electoral_votes) %>% 
  summarise(pct_biden_win = mean(biden_win, na.rm = TRUE), .groups = "drop") %>% 
  mutate(close = pct_biden_win - .5) %>% 
  arrange(abs(close)) %>% 
  select(state, pct_biden_win, electoral_votes) %>% 
  
  # simulating 100,000 elections in each state with the given win probabilities
  
  slice(rep(1:n(), each = 100000)) %>% 
  mutate(biden_win = map_int(pct_biden_win, ~ rbinom(1, 1, .)),
         id = rep(1:100000, times = 50),
         biden_ev = ifelse(biden_win == 1, electoral_votes, 0),
         trump_ev = ifelse(biden_win == 0, electoral_votes, 0))


# getting list of states that Biden won in each simulation

ev_sim_results <- ev_uncertainty %>% 
  group_by(id) %>% 
  mutate(biden_win = as.character(biden_win),
    biden_win = case_when(biden_win == "1" ~ state)) %>% 
  drop_na(biden_win) %>% 
  select(id, biden_win) %>% 
  nest() %>% 
  arrange(id)

# states that Joe Biden actually won

actual_wins <- enos_pred_compare %>% 
  filter(actual_pv2p > 50) %>% 
  pull(state)

# this exact outcome happened 53 / 100,000 times

times <- ev_uncertainty %>% 
  filter(biden_win == 1, 
         state %in% actual_wins) %>% 
  group_by(id) %>% count() %>% 
  filter(n == length(actual_wins)) %>% 
  nrow()

# finding times my point prediction happened

pred_wins <- sims %>% 
  drop_na() %>% 
  mutate(biden_win = ifelse(sim_dvotes_2020 > sim_rvotes_2020, 1, 0)) %>% 
  group_by(state, electoral_votes) %>% 
  summarise(biden_win = mean(biden_win), .groups = "drop") %>% 
  filter(biden_win > 0.5) %>% 
  pull(state)

pred_times <- ev_uncertainty %>% 
  filter(biden_win == 1, 
         state %in% pred_wins) %>% 
  group_by(id) %>% count() %>% 
  filter(n == length(pred_wins)) %>% 
  nrow()

```

The actual Electoral College outcome, with each candidate winning the states that they won on Election Night, occurred in `r times` of my simulations. To put that into perspective, my point prediction occurred in `r pred_times` of my simulations, which equates to `r round(pred_times / 100000, 3)`%. Forecasters cannot predict the election outcome with absolute certainty, but models provide a range of possible scenarios. This model successfully anticipated a close Electoral Race with a large popular vote margin, and the actual outcome occurred more than a handful of times in my simulations.

All in all, I'm quite happy with how this model paralleled with the election outcomes. It only misclassified the winner of GA, NV, and AZ, which were three of the final states called. Even though the model predicted that a Donald Trump victory was more likely in these states, the forecast predicted a close race in those states and gave either candidate a fair shot of winning--Joe Biden won GA, NV, and AZ in 19.2%, 43.9%, and 20.5% of simulations, respectively. In my nationwide election simulations, the exact election outcome occurred in 57 out of 100,000 (0.057%) simulations. Interpreting these probabilities with a frequentist[^frequentist] approach, those probabilities could have very well been correct and we just happened to observe one of the `r times` elections where each candidate won this exact cocktail of states.

```{r compare_states}

# scatterplot of model error

error <- enos_pred_compare %>% 
  mutate(incorrect = ifelse(state %in% c("AZ", "NV", "GA"), TRUE, FALSE)) %>% 
  ggplot(aes(pred_pv2p, actual_pv2p,
             text = paste0("Biden Overperformed in ", state, " by ", round(diff, 3), "%"),
             color = diff, shape = incorrect)) +
  geom_point() +
  geom_abline(slope = 1, intercept = 0) +
  labs(y = "Actual Two-Party Popular Vote Share",
       title = "State-Level Model Error",
       subtitle = "Negative numbers indicate overestimate of Joe Biden's Vote",
       x = "Predicted Two-Party Popular Vote Share",
       color = "Error = \nActual - Predicted",
       shape = "Incorrect Prediction?") +
  theme_hodp() +
  scale_color_gradient2(low = "#BE1E26", mid = "#E2DDDB", high = "#4B5973")

ggplotly(error, tooltip = c("text", "x", "y", "shape"))


# average error... (overestimated Biden's state-level popular votes by an average of 0.392%)

avg_error <- state_pred_compare %>% 
  pull(diff) %>% 
  mean() %>% 
  round(3)

# correlation of 0.9621935 between actual and predicted pv2p

corr <- enos_pred_compare %>% 
  select(actual_pv2p, pred_pv2p) %>% 
  cor() %>% 
  as_tibble() %>% 
  slice(1) %>% 
  pull(2)

# root mean squared error of 3.83554

rmse <- enos_pred_compare %>% 
  mutate(diff = diff^2) %>% 
  pull(diff) %>% 
  mean() %>% 
  sqrt()

# mean error is -0.334

me <- enos_pred_compare %>% 
  pull(diff) %>% 
  mean()

```

With a correlation of `r round(corr, 3)` between the actual and the predicted two-party popular vote for each state, there is an incredibly strong correlation between the actual and predicted state-level two-party vote shares. With that said, there are a few patterns in the inaccuracies:

- On average, Joe Biden underperformed his predicted vote share by `r avg_error` percentage points relative to the forecast. As visible in the below scatterplot, Joe Biden's actual vote share fell short of the model's predictions in the Democrat-leaning states and exceeded the predicted vote share in the Republican-leaning states.

- Despite overpredicting Joe Biden's vote share in most states, the model *underestimated* Joe Biden's performance in the only three misclassified states. Essentially, the model overestimated Joe Biden's vote share in general but underestimated it in the states with incorrect point predictions.

The below maps illustrate the areas with the greatest error. Notice that safe blue and red states such as New York and Louisiana have relatively large errors, while battleground states such as Texas and Ohio have extremely slim errors. For a closer look at the data, the included table contains all of the actual and predicted two-party vote shares for Joe Biden, ordered by the magnitude of the error:

## {.tabset}

### Predicted Actual Results

```{r results_maps}

pred_map <- ggplotly(enos_pred_compare %>% 
  left_join(us_map, by = c("state" = "region")) %>% 
  ggplot(aes(long, lat, group = group, text = paste0("Biden's Predicted Two-Party \nVote Share in ", 
                                                     state, ": ", round(pred_pv2p, 3), "%"))) +
  geom_polygon(aes(fill = pred_pv2p)) +
  scale_fill_gradient2(low = "#BE1E26", high = "#4B5973",
                       midpoint = 50,
                       breaks = c(30, 50, 70),
                       labels = c(30, 50, 70),
                       limits = c(25, 75)) +
  coord_map() +
  labs(x = "",
       y = "",
       fill = "Biden's Two-Party \nVote Share",
       title = "Predicted Vote Shares") +
  theme_hodp() +
  theme(axis.line = element_blank(),
        axis.text = element_blank(),
        axis.text.x = element_blank()),
  tooltip = "text")

actual_map <- ggplotly(enos_pred_compare %>% 
  left_join(us_map, by = c("state" = "region")) %>% 
  ggplot(aes(long, lat, group = group, text = paste0("Biden's Actual Two-Party \nVote Share in ", 
                                                     state, ": ", round(actual_pv2p, 3), "%"))) +
  geom_polygon(aes(fill = actual_pv2p)) +
  scale_fill_gradient2(low = "#BE1E26", high = "#4B5973",
                       midpoint = 50,
                       breaks = c(30, 50, 70),
                       labels = c(30, 50, 70),
                       limits = c(25, 75)) +
  coord_map() +
  labs(x = "",
       y = "",
       fill = "Biden's Two-Party \nVote Share",
       title = "Predicted Versus Actual Results") +
  theme_hodp() +
  theme(axis.line = element_blank(),
        axis.text = element_blank(),
        axis.text.x = element_blank()),
  tooltip = "text")

subplot(pred_map, actual_map, nrows = 2)

```

### Error Map

```{r error_map}

# creating error map

ggplotly(state_pred_compare %>% 
  left_join(us_map, by = c("state" = "region")) %>% 
  ggplot(aes(long, lat, group = group, text = paste0("Difference between actual and \npredicted vote shares in ", 
                                                     state, ": ", round(diff, 3), "%"))) +
  geom_polygon(aes(fill = diff)) +
  scale_fill_gradient2(low = my_red, mid = "white", high = my_blue) +
  coord_map() +
  labs(x = "",
       y = "",
       fill = "Difference Between Biden's \nActual and Predicted \nTwo-Party Vote Share",
       title = "Forecast Error") +
  theme_hodp() +
  theme(axis.line = element_blank(),
        axis.text = element_blank(),
        axis.text.x = element_blank()),
  tooltip = "text") %>% 
 layout(annotations = 
          list(x = 1, y = -0.1, text = "Red indicates that Trump received a greater share of votes than predicted.", 
               showarrow = F, xref='paper', yref='paper', 
               xanchor='right', yanchor='auto', xshift=80, yshift=40,
               font=list(size=15))
 )

```

##

```{r state_error}

# creating a table with the actual and predicted state-level vote shares alongside the errors

scroll_box(state_pred_compare %>% 
  select(state, actual_pv2p, pred_pv2p, diff) %>% 
  arrange(desc(abs(diff))) %>% 
  kable(col.names = c("State", "Actual Democratic Two-Party Vote Share", 
        "Predicted Democratic Two-Party Vote Share", "Error")) %>% 
  kable_styling(bootstrap_options = c("striped", "hover")),
  height = "200px")

```

Since this model was not unilaterally biased like most other forecast models, this model's average error is considerably closer to zero than other popular forecasts, and the errors are more normally distributed around zero:

## {.tabset}

### Comparison Summary Statistics

```{r compare_stats}

{
  # RMSE

  error_rmse <- errors %>% 
    group_by(source) %>% 
    summarise(rmse = sqrt(mean(error^2)),
              .groups = "drop")
  
  # mean error
  
  mean_error <- errors %>% 
    group_by(source) %>% 
    summarise(mean_error = mean(error),
              .groups = "drop")
  
  # classification accuracy
  
  tibble(source = c("538", "economist", "me"), 
         class_acc = c(96, 96, 94),
         missed = c("FL, NC", "FL, NC", "AZ, GA, NV")) %>% 
    inner_join(error_rmse, by = "source") %>% 
    inner_join(mean_error, by = "source") %>% 
    arrange(desc(mean_error)) %>% 
    select(source, mean_error, rmse, class_acc, missed) %>% 
    mutate(source = recode(source, "538" = "FiveThirtyEight",
                           "economist" = "The Economist",
                           "me" = "Kayla Manning")) %>% 
    kable(col.names = c("Model", "Mean Error", "Root Mean Squared Error", 
                        "Classification Accuracy", "Missed States")) %>% 
    kable_styling(bootstrap_options = c("striped", "hover"))

}


```

### Error Histograms

```{r compare_hist}

# generating histograms

ggplotly(errors %>% 
  mutate(source = recode(source, "538" = "FiveThirtyEight",
         "economist" = "The Economist",
         "me" = "Kayla Manning"),
         source = fct_relevel(as_factor(source), "Kayla Manning")) %>% 
  ggplot(aes(error)) +
  geom_density(fill = my_red, alpha = 0.5) +
  geom_histogram(aes(y = after_stat(count / sum(count))), bins = 10, fill = my_red) + 
  geom_vline(xintercept = 0, linetype = "dashed") +
  facet_wrap(~ source) +
  labs(x = "Difference Between Biden's Actual and Predicted \nTwo-Party Vote Share",
       y = "Count",
       title = "Error Distributions by Forecast") +
  theme_hodp()) 

```

##

### Hypothesis for Inaccuracies

As with any forecast model that incorporated polls, this forecast would have benefited from improved polling accuracy. Unfortunately, I do not control the polling methodology, so I must improve my model in other ways. In an effort to minimize the impact of biased polls, I applied an aggressive weighting scheme based on FiveThirtyEight's pollster grades. Despite of these efforts, the model still produced extreme predictions in either direction, with a more favorable predictions for Biden in the liberal states and more favorable predictions for Trump in conservative states. The diverging direction of the inaccuracies leads me to consider other potential causes for the inaccuracies and potential improvements for future iterations of this model. Since this model was not unilaterally biased, it leads to me believe that perhaps this model did not pick up on states trending towards purple. Many of the largest errors came from states in the "reliably blue" and "reliably red" pools for the model construction. While larger errors in these states did not lead to incorrect classification, it is important to identify why my model predicted greater polarization than what played out on election day.

One hypothesis for the polarized predictions is that this model lacked a variable that directly considers the partisan trends within the state. This model neglected to pick up on the magnitude of changing views in states such as Arizona and Georgia, both of which voted for Trump in 2016 yet voted for Biden in 2020.[^good-shifts] To account for this in 2024 and beyond, I could include a variable that captures shifting partisanship within a state between elections. In this model, I attempted to use demographic changes as a proxy for this, but a more direct variable might work better. I plan on incorporating a "difference in Democratic vote share" variable in future iterations of this model, which looks at the difference in the share of that state's two-party popular vote in the previous two elections.

### Proposed Test to Assess Hypothesis

To assess this hypothesis, I could reconstruct the model, following the same procedures as outlined in my [final prediction](https://kayla-manning.github.io/gov1347/posts/final.html). I would use the same data from 1992-2016 and include the new variable that captures the state-level changes in voting patterns between elections. Once I have constructed this model, I would follow a series of steps to assess its validity:

1. First and foremost, I would assess the statistical significance of the partisan change coefficients for each of the pooled models.
2. Then, I would assess the out-of-sample fit with a leave-one-out cross-validation and compare the classification accuracy to my official 2020 forecast.
3. If both of those steps support the strength of this new model, I would forecast the 2020 results using this year's data. To remain consistent between the two models, I would not use polls from after 3 PM EST on November 1, which is the last time I ran the previous forecast model.
4. Finally, I would compare this model's 2020 forecast to my previous model. 

These assessments should provide enough metrics to determine which model performs better in- and out-of-sample. If this model more accurately predicted the state-level outcomes, then I know that my 2024 should resemble this newer model. However, if my previous model performed better in the leave-one-out classification and on the 2020 data, then I would stick with my original, more parsimonious model for the future.


### Improvements for Future Iterations

Aside from the lack of a variable to capture shifting partisan alignment within states, I also plan to make several methodological changes to this model for the future. I touched on many of these in greater detail in my [final prediction](https://kayla-manning.github.io/gov1347/posts/final.html) post, but here is a brief overview:

- This model does not include Washington D.C., so I manually added its 3 electoral votes after forecasting the vote shares for the 50 states. Ideally, I would find the necessary data to include D.C. in my forecast.
- Also due to the absence of appropriate data, this model allocates the electoral votes from Maine and Nebraska on a winner-take-all basis rather than following the congressional district method, as they do in reality. Again, future iterations would ideally include district-level data for these states.
- I need to improve my methodology for varying voter turnout and probabilities:
  * This model varied voter turnout and partisan probabilities independently by simply drawing from a normal distribution. A more sophisticated model in the future would introduce some correlation between geographies, demographic groups, and ideologies. 
  * Moreover, since I drew these probabilities from a normal distribution, some states could have negative probabilities if the initial probability for voting for a particular party was extremely low (e.g. voting Republican in Hawaii). I took the absolute value of these draws to ensure all probabilities were positive, but this introduced some extreme variation in states that had an extremely low probability of voting for one party. For example, the confidence intervals for Republican votes in Hawaii were unrealistically wide since the negative vote probabilities became positive probabilities of a larger magnitude than what was actually likely in the normal distribution. I must find a better method to restrict the domain in future iterations of this model.
- Lastly, I classified states based on their 2020 ideologies. In the future, I would like to set a rule for classifying each state for every election, rather than relying solely on the 2020 classification by the [New York Times](https://www.nytimes.com/interactive/2020/us/elections/electoral-college-battleground-states.html). For example, this model considered Colorado as a "blue state" for all years based on its 2020 classification, but it was either a "red state" or "battleground state" in most of the previous elections in the data. In its current condition, the "blue state" model was constructed with all Colorado data from 1992-2018. Ideally, I would use Colorado data from the years it was considered a "battleground state" to construct the "battleground" model, the years it was a "red state" to construct the "red" model, and the years it was a "blue state" to construct the "blue" model.

### Conclusion

While my forecast failed to predict the election outcomes with absolute precision, this model correctly projected a relatively close race in the Electoral College with a larger margin in the popular vote. Furthermore, the outcomes of November 3 all reasonably match the probabilities assigned by the model. Even in GA, NV, and AZ--the three misclassified states--the actual vote shares were not too far from the predictions, and the simulations gave both candidates a fair probability of winning all three of those states. Despite having predicted this election exceptionally well when many models did not, future iterations of this model must do a better job at accounting for partisan shifts within states.


---------------------------------------------------------------------

[^frequentist]: Unlike rolling dice, we cannot experience multiple occurrences of the same election to uncover the true probability of each event. Frequentist probability describes the relative frequency of an event in many trials; conducting many simulations in my model took a frequentist approach to uncover the probability of each outcome. However, we can never really know if any of the probabilities were correct because the 2020 election only happened once (thank goodness!). Trying to say whether or not a probabilistic forecast was *correct* is like rolling a "six" on a single die and concluding that your prior probabilities of 1/6 for rolling a 6 and 5/6 for rolling anything else were incorrect because you observed the less probable outcome on a single iteration.

[^good-shifts]: However, any changes would have to keep in mind that FL, OH, WI, etc. were more conservative than most forecasts anticipated, and this model correctly anticipated the winner in these highly contentious battleground states.

